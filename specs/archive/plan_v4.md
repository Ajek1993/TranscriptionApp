# Plan v4 - Nowe funkcje transkrypcji i TTS

## PrzeglÄ…d

Dodanie nowych funkcji do projektu transkrypcji:
1. Pobieranie tylko audio z YouTube (bez transkrypcji)
2. WhisperX jako nowy silnik transkrypcji
3. Coqui TTS jako alternatywny silnik TTS
4. Piper TTS jako kolejny silnik TTS

## Krok 1: Pobieranie tylko audio (--download-audio-only)

### Cel
UmoÅ¼liwienie pobierania audio z YouTube bez wykonywania transkrypcji, podobnie jak istniejÄ…ca flaga `--download` dla wideo.

### Implementacja

#### 1.1. Dodanie nowej flagi CLI
- Lokalizacja: `transcribe.py` - sekcja argument parsera
- DodaÄ‡ w grupie "Podstawowe opcje":
  ```python
  parser.add_argument('--download-audio-only', action='store_true',
                      help='Pobierz tylko audio z YouTube (bez transkrypcji)')
  ```

#### 1.2. Logika pobierania audio
- Lokalizacja: `transcribe.py` - funkcja gÅ‚Ã³wna `main()`
- Po walidacji URL YouTube:
  - SprawdziÄ‡ flagÄ™ `args.download_audio_only`
  - JeÅ›li ustawiona, wywoÅ‚aÄ‡ `download_audio_from_youtube()`
  - ZapisaÄ‡ audio jako `{VIDEO_ID}.wav` w bieÅ¼Ä…cym katalogu
  - WyÅ›wietliÄ‡ komunikat o sukcesie z lokalizacjÄ… pliku
  - ZakoÅ„czyÄ‡ program (bez transkrypcji)

#### 1.3. Parametry jakoÅ›ci audio
- Opcjonalnie dodaÄ‡ flagÄ™ `--audio-quality` z wartoÅ›ciami:
  - `best` (domyÅ›lnie) - najlepsza dostÄ™pna jakoÅ›Ä‡
  - `192` - 192 kbps
  - `128` - 128 kbps
  - `96` - 96 kbps

#### 1.4. Aktualizacja dokumentacji
- README.md - dodaÄ‡ sekcjÄ™ "Pobieranie audio z YouTube"
- PrzykÅ‚ady uÅ¼ycia:
  ```bash
  # Pobierz audio w najlepszej jakoÅ›ci
  python transcribe.py --download-audio-only "https://youtube.com/watch?v=VIDEO_ID"

  # Pobierz z niÅ¼szÄ… jakoÅ›ciÄ… (mniejszy plik)
  python transcribe.py --download-audio-only "URL" --audio-quality 128
  ```

### ZaleÅ¼noÅ›ci
- Brak nowych zaleÅ¼noÅ›ci (wykorzystuje istniejÄ…ce yt-dlp i ffmpeg)

### Testy
- Test pobierania krÃ³tkiego wideo z YouTube
- Test z rÃ³Å¼nymi jakoÅ›ciami audio
- Weryfikacja formatu wyjÅ›ciowego (WAV, 16kHz mono)

---

## Krok 2: WhisperX jako nowy engine transkrypcji

### Cel
Dodanie WhisperX jako trzeciego silnika transkrypcji, oferujÄ…cego lepszÄ… dokÅ‚adnoÅ›Ä‡ timestampÃ³w i alignment sÅ‚Ã³w.

### Implementacja

#### 2.1. Instalacja zaleÅ¼noÅ›ci
- DodaÄ‡ do `requirements.txt`:
  ```
  whisperx>=3.1.1
  ```
- Opcjonalne zaleÅ¼noÅ›ci dla alignment:
  ```
  phonemizer>=3.2.1
  ```

#### 2.2. Dodanie opcji CLI
- RozszerzyÄ‡ `--engine` o nowÄ… wartoÅ›Ä‡:
  ```python
  parser.add_argument('--engine', choices=['faster-whisper', 'whisper', 'whisperx'],
                      default='faster-whisper',
                      help='Silnik transkrypcji (faster-whisper/whisper/whisperx)')
  ```

#### 2.3. Funkcja transkrypcji WhisperX
- Lokalizacja: nowa funkcja `transcribe_with_whisperx()`
- Parametry:
  - `audio_path`: Å›cieÅ¼ka do pliku audio
  - `model_size`: rozmiar modelu (tiny/base/small/medium/large)
  - `language`: jÄ™zyk transkrypcji
  - `device`: 'cuda' lub 'cpu'
  - `compute_type`: 'float16' dla GPU, 'int8' dla CPU

#### 2.4. Workflow WhisperX
```python
def transcribe_with_whisperx(audio_path, model_size='base', language='pl', device='cpu', compute_type='int8'):
    """
    Transkrypcja z WhisperX

    Etapy:
    1. Åadowanie modelu WhisperX
    2. Transkrypcja audio
    3. Word-level alignment (opcjonalnie)
    4. Diarization - identyfikacja mÃ³wcÃ³w (opcjonalnie)
    5. ZwrÃ³cenie segmentÃ³w z precyzyjnymi timestampami
    """
    import whisperx

    # Åadowanie modelu
    model = whisperx.load_model(model_size, device, compute_type=compute_type, language=language)

    # Transkrypcja
    audio = whisperx.load_audio(audio_path)
    result = model.transcribe(audio, batch_size=16)

    # Word-level alignment (jeÅ›li dostÄ™pny model dla jÄ™zyka)
    if args.whisperx_align:
        model_a, metadata = whisperx.load_align_model(language_code=language, device=device)
        result = whisperx.align(result["segments"], model_a, metadata, audio, device)

    # Konwersja do formatu kompatybilnego z resztÄ… kodu
    segments = convert_whisperx_segments(result)

    return segments
```

#### 2.5. Dodatkowe flagi dla WhisperX
- `--whisperx-align`: WÅ‚Ä…cz word-level alignment
- `--whisperx-diarize`: WÅ‚Ä…cz speaker diarization (wymaga HuggingFace token)
- `--whisperx-min-speakers`: Minimalna liczba mÃ³wcÃ³w (dla diarization)
- `--whisperx-max-speakers`: Maksymalna liczba mÃ³wcÃ³w (dla diarization)

#### 2.6. Integracja w gÅ‚Ã³wnym flow
- Lokalizacja: `main()` - etap 3 (transkrypcja)
- Warunek:
  ```python
  if args.engine == 'whisperx':
      segments = transcribe_with_whisperx(chunk_path, args.model, args.language, device, compute_type)
  elif args.engine == 'faster-whisper':
      # istniejÄ…cy kod
  elif args.engine == 'whisper':
      # istniejÄ…cy kod
  ```

#### 2.7. Aktualizacja dokumentacji
- README.md - rozszerzyÄ‡ sekcjÄ™ "Silniki transkrypcji"
- DodaÄ‡ informacje o WhisperX:
  - Zalety: najlepsza dokÅ‚adnoÅ›Ä‡ timestampÃ³w, word-level alignment, speaker diarization
  - Wady: wolniejszy niÅ¼ faster-whisper, wiÄ™ksze zuÅ¼ycie pamiÄ™ci
  - Kiedy uÅ¼ywaÄ‡: gdy potrzebna jest najwyÅ¼sza precyzja timestampÃ³w lub identyfikacja mÃ³wcÃ³w

### ZaleÅ¼noÅ›ci
- `whisperx>=3.1.1`
- `phonemizer>=3.2.1` (opcjonalnie, dla alignment)
- `pyannote.audio` (opcjonalnie, dla diarization)

### Testy
- Test transkrypcji krÃ³tkiego audio
- Test z word-level alignment
- Test na rÃ³Å¼nych jÄ™zykach (pl, en)
- PorÃ³wnanie dokÅ‚adnoÅ›ci timestampÃ³w z faster-whisper

---

## Krok 3: Coqui TTS jako alternatywny engine TTS (--tts-engine coqui)

### Cel
Dodanie Coqui TTS jako wysokiej jakoÅ›ci alternatywy dla Microsoft Edge TTS, z moÅ¼liwoÅ›ciÄ… lokalnego generowania mowy.

### Implementacja

#### 3.1. Instalacja zaleÅ¼noÅ›ci
- DodaÄ‡ do `requirements.txt`:
  ```
  TTS>=0.22.0
  ```

#### 3.2. Dodanie flagi --tts-engine
- Lokalizacja: argument parser - grupa "Dubbing i TTS"
  ```python
  parser.add_argument('--tts-engine', choices=['edge', 'coqui', 'piper'],
                      default='edge',
                      help='Silnik TTS (edge/coqui/piper)')
  ```

#### 3.3. Funkcja generowania TTS z Coqui
- Lokalizacja: nowa funkcja `generate_tts_coqui()`
```python
def generate_tts_coqui(text, output_path, voice_model='tts_models/pl/mai_female/vits',
                       speaker=None, language='pl', speed=1.0):
    """
    Generowanie TTS z Coqui TTS

    Args:
        text: tekst do syntezy
        output_path: Å›cieÅ¼ka wyjÅ›ciowa (WAV)
        voice_model: model Coqui TTS
        speaker: ID mÃ³wcy (dla modeli multi-speaker)
        language: jÄ™zyk (dla modeli multi-language)
        speed: prÄ™dkoÅ›Ä‡ mowy (1.0 = normalna)
    """
    from TTS.api import TTS

    # Inicjalizacja TTS (cache model)
    if not hasattr(generate_tts_coqui, 'tts_model'):
        generate_tts_coqui.tts_model = TTS(model_name=voice_model, progress_bar=False)

    tts = generate_tts_coqui.tts_model

    # Generowanie
    if speaker:
        tts.tts_to_file(text=text, file_path=output_path, speaker=speaker, speed=speed)
    elif language:
        tts.tts_to_file(text=text, file_path=output_path, language=language, speed=speed)
    else:
        tts.tts_to_file(text=text, file_path=output_path, speed=speed)
```

#### 3.4. DostÄ™pne modele Coqui dla polskiego
Predefiniowane modele w kodzie:
- `tts_models/pl/mai_female/vits` - polski gÅ‚os Å¼eÅ„ski (domyÅ›lny)
- `tts_models/multilingual/multi-dataset/your_tts` - multi-language (zawiera polski)
- `tts_models/multilingual/multi-dataset/xtts_v2` - XTTS v2 (najlepsza jakoÅ›Ä‡, wymaga GPU)

#### 3.5. Dodatkowe flagi dla Coqui
```python
parser.add_argument('--coqui-model', default='tts_models/pl/mai_female/vits',
                    help='Model Coqui TTS (domyÅ›lnie: polski Å¼eÅ„ski)')
parser.add_argument('--coqui-speaker', help='ID mÃ³wcy (dla modeli multi-speaker)')
parser.add_argument('--coqui-list-models', action='store_true',
                    help='WyÅ›wietl dostÄ™pne modele Coqui TTS')
```

#### 3.6. Funkcja listowania modeli
```python
def list_coqui_models():
    """WyÅ›wietl wszystkie dostÄ™pne modele Coqui TTS"""
    from TTS.api import TTS
    models = TTS().list_models()

    print("\n=== DostÄ™pne modele Coqui TTS ===\n")

    # Filtruj modele polskie
    polish_models = [m for m in models if '/pl/' in m]
    if polish_models:
        print("Modele polskie:")
        for model in polish_models:
            print(f"  - {model}")

    # Modele wielojÄ™zyczne
    multi_models = [m for m in models if 'multilingual' in m]
    if multi_models:
        print("\nModele wielojÄ™zyczne (zawierajÄ… polski):")
        for model in multi_models[:5]:  # PokaÅ¼ top 5
            print(f"  - {model}")
```

#### 3.7. Integracja w dubbing workflow
- Lokalizacja: funkcja `generate_dubbing()` lub nowa `generate_dubbing_v2()`
- DodaÄ‡ warunek:
  ```python
  if args.tts_engine == 'edge':
      # istniejÄ…cy kod z edge-tts
      await generate_segment_tts_edge(...)
  elif args.tts_engine == 'coqui':
      # nowy kod z Coqui
      generate_tts_coqui(segment_text, segment_path,
                         voice_model=args.coqui_model,
                         speed=speed_factor)
  elif args.tts_engine == 'piper':
      # kod dla Piper (krok 4)
      generate_tts_piper(...)
  ```

#### 3.8. ObsÅ‚uga przyspieszania TTS
- Coqui obsÅ‚uguje natywne przyspieszanie przez parametr `speed`
- WykorzystaÄ‡ istniejÄ…cÄ… logikÄ™ obliczania `speed_factor` z edge-tts
- Max przyspieszenie: 1.5x (50%)

#### 3.9. Aktualizacja dokumentacji
- README.md - nowa sekcja "Silniki TTS"
- Tabela porÃ³wnawcza:

| Engine | JakoÅ›Ä‡ | SzybkoÅ›Ä‡ | Wymaga internetu | JÄ™zyki | GPU |
|--------|--------|----------|------------------|--------|-----|
| edge   | Dobra  | Szybka   | Tak              | Wiele  | Nie |
| coqui  | Bardzo dobra | Åšrednia | Nie | Wiele | Opcjonalnie |
| piper  | Dobra  | Bardzo szybka | Nie | Wiele | Nie |

### ZaleÅ¼noÅ›ci
- `TTS>=0.22.0` (Coqui TTS)
- PyTorch (juÅ¼ wymagany dla Whisper)

### Testy
- Test generowania pojedynczego segmentu
- Test z rÃ³Å¼nymi modelami polskimi
- Test przyspieszania TTS
- PorÃ³wnanie jakoÅ›ci z edge-tts

---

## Krok 4: Piper TTS jako engine TTS (--tts-engine piper)

### Cel
Dodanie Piper TTS jako szybkiej i lekkiej alternatywy, dziaÅ‚ajÄ…cej offline z maÅ‚ym footprintem pamiÄ™ci.

### Implementacja

#### 4.1. Instalacja zaleÅ¼noÅ›ci
- DodaÄ‡ do `requirements.txt`:
  ```
  piper-tts>=1.2.0
  ```
- Alternatywnie: uÅ¼yÄ‡ binarnego Piper (bez Python wrapper)

#### 4.2. Funkcja generowania TTS z Piper
```python
def generate_tts_piper(text, output_path, model_path=None, speaker=0, speed=1.0):
    """
    Generowanie TTS z Piper

    Args:
        text: tekst do syntezy
        output_path: Å›cieÅ¼ka wyjÅ›ciowa (WAV)
        model_path: Å›cieÅ¼ka do modelu Piper (.onnx)
        speaker: ID mÃ³wcy (dla modeli multi-speaker)
        speed: prÄ™dkoÅ›Ä‡ mowy (0.5-2.0)
    """
    import subprocess
    import json

    # DomyÅ›lny model polski (pobierz jeÅ›li nie istnieje)
    if not model_path:
        model_path = ensure_piper_model('pl_PL-darkman-medium')

    config_path = model_path.replace('.onnx', '.onnx.json')

    # Przygotuj tekst (escape)
    text_escaped = text.replace('"', '\\"')

    # WywoÅ‚aj Piper przez subprocess
    cmd = [
        'piper',
        '--model', model_path,
        '--config', config_path,
        '--output_file', output_path,
        '--speaker', str(speaker),
        '--length_scale', str(1.0 / speed)  # Piper uÅ¼ywa length_scale (1.0 = normalnie)
    ]

    # PrzekaÅ¼ tekst przez stdin
    process = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,
                               stderr=subprocess.PIPE, text=True)
    stdout, stderr = process.communicate(input=text_escaped)

    if process.returncode != 0:
        raise RuntimeError(f"Piper TTS error: {stderr}")
```

#### 4.3. Pobieranie modeli Piper
```python
def ensure_piper_model(model_name='pl_PL-darkman-medium'):
    """
    SprawdÅº i pobierz model Piper jeÅ›li nie istnieje

    Modele polskie Piper:
    - pl_PL-darkman-medium (mÄ™ski, Å›rednia jakoÅ›Ä‡)
    - pl_PL-mls_6892-low (niÅ¼sza jakoÅ›Ä‡, szybszy)
    """
    import urllib.request
    from pathlib import Path

    models_dir = Path.home() / '.local' / 'share' / 'piper' / 'models'
    models_dir.mkdir(parents=True, exist_ok=True)

    model_path = models_dir / f'{model_name}.onnx'
    config_path = models_dir / f'{model_name}.onnx.json'

    # SprawdÅº czy model istnieje
    if model_path.exists() and config_path.exists():
        return str(model_path)

    # Pobierz model
    print(f"Pobieranie modelu Piper: {model_name}")
    base_url = f'https://huggingface.co/rhasspy/piper-voices/resolve/main/{model_name.replace("-", "/")}'

    urllib.request.urlretrieve(f'{base_url}/{model_name}.onnx', model_path)
    urllib.request.urlretrieve(f'{base_url}/{model_name}.onnx.json', config_path)

    print(f"Model pobrano do: {model_path}")
    return str(model_path)
```

#### 4.4. Dodatkowe flagi dla Piper
```python
parser.add_argument('--piper-model', default='pl_PL-darkman-medium',
                    help='Model Piper TTS (domyÅ›lnie: polski mÄ™ski)')
parser.add_argument('--piper-speaker', type=int, default=0,
                    help='ID mÃ³wcy dla modeli multi-speaker (domyÅ›lnie: 0)')
```

#### 4.5. DostÄ™pne modele Piper dla polskiego
DodaÄ‡ do dokumentacji:
- `pl_PL-darkman-medium` - mÄ™ski, Å›rednia jakoÅ›Ä‡ (domyÅ›lny)
- `pl_PL-mls_6892-low` - niÅ¼sza jakoÅ›Ä‡, szybszy

#### 4.6. Integracja w dubbing workflow
- DodaÄ‡ do warunku w `generate_dubbing_v2()`:
  ```python
  elif args.tts_engine == 'piper':
      generate_tts_piper(segment_text, segment_path,
                         model_path=None if not args.piper_model else ensure_piper_model(args.piper_model),
                         speaker=args.piper_speaker,
                         speed=speed_factor)
  ```

#### 4.7. ObsÅ‚uga offline
- Piper dziaÅ‚a w 100% offline po pobraniu modelu
- Modele sÄ… maÅ‚e (20-100 MB)
- Komunikat przy pierwszym uÅ¼yciu: "Pobieranie modelu Piper (jednorazowo)..."

#### 4.8. Aktualizacja dokumentacji
- README.md - rozszerzyÄ‡ sekcjÄ™ "Silniki TTS"
- DodaÄ‡ przykÅ‚ady:
  ```bash
  # Dubbing z Coqui (lepsza jakoÅ›Ä‡, wolniejsze)
  python transcribe.py --local "film.mp4" --dub --tts-engine coqui

  # Dubbing z Piper (szybszy, offline)
  python transcribe.py --local "film.mp4" --dub --tts-engine piper

  # Coqui z wÅ‚asnym modelem
  python transcribe.py "URL" --dub --tts-engine coqui --coqui-model tts_models/multilingual/multi-dataset/xtts_v2

  # Piper z niÅ¼szÄ… jakoÅ›ciÄ… (szybsze)
  python transcribe.py "URL" --dub --tts-engine piper --piper-model pl_PL-mls_6892-low
  ```

### ZaleÅ¼noÅ›ci
- `piper-tts>=1.2.0` (Python wrapper)
- Lub binarne Piper: https://github.com/rhasspy/piper/releases

### Testy
- Test pobierania modelu przy pierwszym uÅ¼yciu
- Test generowania TTS offline
- Test rÃ³Å¼nych modeli polskich
- PorÃ³wnanie szybkoÅ›ci z edge-tts i coqui

---

## Podsumowanie zmian

### Nowe flagi CLI

#### Pobieranie audio
```
--download-audio-only     Pobierz tylko audio z YouTube (bez transkrypcji)
--audio-quality QUALITY   JakoÅ›Ä‡ audio (best/192/128/96)
```

#### WhisperX
```
--engine whisperx         UÅ¼yj WhisperX jako silnika transkrypcji
--whisperx-align          WÅ‚Ä…cz word-level alignment
--whisperx-diarize        WÅ‚Ä…cz speaker diarization
--whisperx-min-speakers N Minimalna liczba mÃ³wcÃ³w
--whisperx-max-speakers N Maksymalna liczba mÃ³wcÃ³w
```

#### Silniki TTS
```
--tts-engine ENGINE       Silnik TTS (edge/coqui/piper)

# Coqui-specific
--coqui-model MODEL       Model Coqui TTS
--coqui-speaker ID        ID mÃ³wcy (multi-speaker models)
--coqui-list-models       Lista dostÄ™pnych modeli Coqui

# Piper-specific
--piper-model MODEL       Model Piper TTS
--piper-speaker ID        ID mÃ³wcy (multi-speaker models)
```

### Nowe zaleÅ¼noÅ›ci w requirements.txt
```
# Krok 2
whisperx>=3.1.1
phonemizer>=3.2.1

# Krok 3
TTS>=0.22.0

# Krok 4
piper-tts>=1.2.0
```

### KolejnoÅ›Ä‡ implementacji
1. **Krok 1** (najprostszy): Pobieranie audio - ~2-3 godziny
2. **Krok 2** (Å›redni): WhisperX - ~4-6 godzin
3. **Krok 3** (Å›redni): Coqui TTS - ~4-6 godzin
4. **Krok 4** (Å›redni): Piper TTS - ~4-6 godzin

### Struktura plikÃ³w po zmianach
```
transcribe.py                 # GÅ‚Ã³wny plik (rozszerzony)
â”œâ”€â”€ Sekcja 1: Imports         # DodaÄ‡: whisperx, TTS, piper
â”œâ”€â”€ Sekcja 2: Funkcje TTS
â”‚   â”œâ”€â”€ generate_tts_edge()   # IstniejÄ…ce
â”‚   â”œâ”€â”€ generate_tts_coqui()  # NOWE - Krok 3
â”‚   â””â”€â”€ generate_tts_piper()  # NOWE - Krok 4
â”œâ”€â”€ Sekcja 3: Funkcje transkrypcji
â”‚   â”œâ”€â”€ transcribe_with_faster_whisper()  # IstniejÄ…ce
â”‚   â”œâ”€â”€ transcribe_with_whisper()         # IstniejÄ…ce
â”‚   â””â”€â”€ transcribe_with_whisperx()        # NOWE - Krok 2
â”œâ”€â”€ Sekcja 4: Funkcje pomocnicze
â”‚   â”œâ”€â”€ ensure_piper_model()  # NOWE - Krok 4
â”‚   â””â”€â”€ list_coqui_models()   # NOWE - Krok 3
â””â”€â”€ Sekcja 5: Main workflow   # Rozszerzone warunki

requirements.txt              # Rozszerzony
README.md                     # Aktualizowany w kaÅ¼dym kroku
specs/plan_v4.md             # Ten plik
```

### KompatybilnoÅ›Ä‡ wsteczna
- âœ… Wszystkie istniejÄ…ce flagi dziaÅ‚ajÄ… bez zmian
- âœ… DomyÅ›lne wartoÅ›ci zachowane (`--engine faster-whisper`, `--tts-engine edge`)
- âœ… Brak breaking changes w API

### Testy integracyjne (po wszystkich krokach)
```bash
# Test 1: Audio download
python transcribe.py --download-audio-only "URL"

# Test 2: WhisperX transkrypcja
python transcribe.py --local "audio.wav" --engine whisperx

# Test 3: Coqui dubbing
python transcribe.py --local "video.mp4" --dub --tts-engine coqui

# Test 4: Piper dubbing (offline)
python transcribe.py --local "video.mp4" --dub --tts-engine piper

# Test 5: PeÅ‚ny workflow z WhisperX + Coqui
python transcribe.py "URL" --engine whisperx --whisperx-align --dub --tts-engine coqui

# Test 6: PorÃ³wnanie silnikÃ³w TTS
python transcribe.py --local "test.mp4" --dub --tts-engine edge -o test_edge.mp4
python transcribe.py --local "test.mp4" --dub --tts-engine coqui -o test_coqui.mp4
python transcribe.py --local "test.mp4" --dub --tts-engine piper -o test_piper.mp4
```

---

## ZaÅ‚Ä…czniki

### A. PorÃ³wnanie silnikÃ³w transkrypcji

| Silnik         | SzybkoÅ›Ä‡ | DokÅ‚adnoÅ›Ä‡ | Timestamps | Word-level | Diarization | PamiÄ™Ä‡ |
|----------------|----------|------------|------------|------------|-------------|--------|
| faster-whisper | âš¡âš¡âš¡    | â­â­â­     | â­â­       | âŒ         | âŒ          | ğŸ’¾ğŸ’¾   |
| whisper        | âš¡âš¡      | â­â­â­â­   | â­â­       | âŒ         | âŒ          | ğŸ’¾ğŸ’¾ğŸ’¾ |
| whisperx       | âš¡âš¡      | â­â­â­â­â­ | â­â­â­â­â­ | âœ…         | âœ…          | ğŸ’¾ğŸ’¾ğŸ’¾ |

### B. PorÃ³wnanie silnikÃ³w TTS

| Silnik | JakoÅ›Ä‡ | SzybkoÅ›Ä‡ | Offline | JÄ™zyki | GÅ‚osy | Rozmiar |
|--------|--------|----------|---------|--------|-------|---------|
| edge   | â­â­â­ | âš¡âš¡âš¡   | âŒ      | 70+    | 200+  | 0 MB    |
| coqui  | â­â­â­â­â­ | âš¡âš¡   | âœ…      | 40+    | 100+  | 100-500 MB |
| piper  | â­â­â­ | âš¡âš¡âš¡âš¡ | âœ…      | 30+    | 50+   | 20-100 MB |

### C. PrzykÅ‚adowe modele Coqui TTS dla polskiego

```python
COQUI_POLISH_MODELS = {
    'mai_female': 'tts_models/pl/mai_female/vits',  # Najlepsza jakoÅ›Ä‡, kobieta
    'multilingual_xtts': 'tts_models/multilingual/multi-dataset/xtts_v2',  # Multi-language, najlepsza jakoÅ›Ä‡ ogÃ³lna
    'your_tts': 'tts_models/multilingual/multi-dataset/your_tts',  # Multi-language, szybszy
}
```

### D. PrzykÅ‚adowe modele Piper TTS dla polskiego

```python
PIPER_POLISH_MODELS = {
    'darkman_medium': 'pl_PL-darkman-medium',  # MÄ™ski, Å›rednia jakoÅ›Ä‡ (domyÅ›lny)
    'mls_low': 'pl_PL-mls_6892-low',  # NiÅ¼sza jakoÅ›Ä‡, szybszy
}
```

---

**Status:** Gotowy do implementacji
**Priorytet:** Åšredni
**Estimated effort:** ~16-20 godzin (wszystkie 4 kroki)
